{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3182f9f",
   "metadata": {},
   "source": [
    "### **Cross Validation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path().resolve().parent\n",
    "    \n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.utils import read_processed_data, config_loader, get_logger\n",
    "\n",
    "\n",
    "log = get_logger()\n",
    "config = config_loader()\n",
    "path_svc = os.path.join(project_root, config['cv_result'], 'svc_results.csv')\n",
    "path_knn = os.path.join(project_root, config['cv_result'], 'knn_results.csv')\n",
    "path_rf = os.path.join(project_root, config['cv_result'], 'rf_results.csv')\n",
    "path_xgb = os.path.join(project_root, config['cv_result'], 'xgb_results.csv')\n",
    "\n",
    "cv_result = read_processed_data(path=path_svc, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe546fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the cv_result DataFrame\n",
    "print(\"Shape of cv_result:\", cv_result.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(cv_result.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(cv_result.describe(include='all'))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(cv_result.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cv_result['param_C'], cv_result['mean_test_accuracy'], marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularizaton Parameter)')\n",
    "plt.title(\"SVC: Accuracy vs. C\")\n",
    "plt.ylabel('Mean CV Accuracy')\n",
    "plt.grid(True)\n",
    "plt.savefig('svc_accuracy_vs_C.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cv_result['param_C'], cv_result['mean_test_f1'], marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularizaton Parameter)')\n",
    "plt.title(\"SVC: F1-Score vs. C\")\n",
    "plt.ylabel('Mean CV F1-Score')\n",
    "plt.grid(True)\n",
    "plt.savefig('svc_f1score_vs_C.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cv_result['param_C'], cv_result['mean_test_roc_auc'], marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularizaton Parameter)')\n",
    "plt.title(\"SVC: ROC-AUC vs. C\")\n",
    "plt.ylabel(\"Mean CV ROC-AUC\")\n",
    "plt.grid(True)\n",
    "plt.savefig('svc_roc_vs_C.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ff43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  KNN Plot..\n",
    "cv_result_knn = read_processed_data(path=path_knn, log=log)\n",
    "\n",
    "metrics = [\n",
    "    ('mean_test_accuracy', 'Accuracy'),\n",
    "    ('mean_test_f1', 'F1 Score'),\n",
    "    ('mean_test_recall', 'Recall'),\n",
    "    ('mean_test_precision', 'Precision'),\n",
    "    ('mean_test_roc_auc', 'ROC-AUC')\n",
    "]\n",
    "\n",
    "for metric, label in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cv_result_knn['param_n_neighbors'], cv_result_knn[metric], marker='o')\n",
    "    plt.xlabel('Number of Neighbors (k)')\n",
    "    plt.ylabel(f'Mean CV {label}')\n",
    "    plt.title(f'KNN: {label} vs. Number of Neighbors')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'knn_{metric}_vs_k.png', dpi=500)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb744fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Plot..\n",
    "cv_result_xgb = read_processed_data(path=path_xgb, log=log)\n",
    "\n",
    "metrics = [\n",
    "    ('accuracy', 'mean_test_accuracy', 'mean_train_accuracy'),\n",
    "    ('f1', 'mean_test_f1', 'mean_train_f1'),\n",
    "    ('recall', 'mean_test_recall', 'mean_train_recall'),\n",
    "    ('precision', 'mean_test_precision', 'mean_train_precision'),\n",
    "    ('roc_auc', 'mean_test_roc_auc', 'mean_train_roc_auc')\n",
    "]\n",
    "\n",
    "x_param = 'param_n_estimators'  \n",
    "\n",
    "for label, test_metric, train_metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[test_metric], marker='o', label='Mean Test')\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[train_metric], marker='s', label='Mean Train')\n",
    "    plt.xlabel(x_param.replace('param_', '').replace('_', ' ').title())\n",
    "    plt.ylabel(label.title())\n",
    "    plt.title(f'XGBoost: Train vs. Test {label.title()} vs. {x_param.replace(\"param_\", \"\").title()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'xgb_{label}_train_test_vs_{x_param}.png', dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "x_param = 'param_max_depth'  \n",
    "\n",
    "for label, test_metric, train_metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[test_metric], marker='o', label='Mean Test')\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[train_metric], marker='s', label='Mean Train')\n",
    "    plt.xlabel(x_param.replace('param_', '').replace('_', ' ').title())\n",
    "    plt.ylabel(label.title())\n",
    "    plt.title(f'XGBoost: Train vs. Test {label.title()} vs. {x_param.replace(\"param_\", \"\").title()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'xgb_{label}_train_test_vs_{x_param}.png', dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "x_param = 'param_learning_rate'  \n",
    "\n",
    "for label, test_metric, train_metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[test_metric], marker='o', label='Mean Test')\n",
    "    plt.plot(cv_result_xgb[x_param], cv_result_xgb[train_metric], marker='s', label='Mean Train')\n",
    "    plt.xlabel(x_param.replace('param_', '').replace('_', ' ').title())\n",
    "    plt.ylabel(label.title())\n",
    "    plt.title(f'XGBoost: Train vs. Test {label.title()} vs. {x_param.replace(\"param_\", \"\").title()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'xgb_{label}_train_test_vs_{x_param}.png', dpi=500)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Plot..\n",
    "cv_result_rf = read_processed_data(path=path_rf, log=log)\n",
    "\n",
    "metrics = [\n",
    "    ('mean_test_accuracy', 'Accuracy'),\n",
    "    ('mean_test_f1', 'F1 Score'),\n",
    "    ('mean_test_recall', 'Recall'),\n",
    "    ('mean_test_precision', 'Precision'),\n",
    "    ('mean_test_roc_auc', 'ROC-AUC')\n",
    "]\n",
    "\n",
    "for metric, label in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cv_result_rf['param_n_estimators'], cv_result_rf[metric], marker='o')\n",
    "    plt.xlabel('Number of Trees (n_estimators)')\n",
    "    plt.ylabel(f'Mean CV {label}')\n",
    "    plt.title(f'XGB: {label} vs. Number of Trees')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'xgb_{metric}_vs_estimators.png', dpi=500)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
